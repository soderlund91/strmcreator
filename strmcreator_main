#!/usr/bin/env python3
import re
import requests
import time
import os
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# ================= USER SETTINGS =================

M3U_URL = "REPLACE_ME" # Link to you m3u file. (keep the brackets "")
LOCAL_M3U_FILE = "playlist.m3u8" #Name of downloaded m3u-file. Used as fallback if no connection

OUTPUT_BASE = "/home/user/videos" #Base directory for.strm-files

WHITELIST_FILE = "group_whitelist.txt" #Name of whitlist. Run strmcreator_wl.py first. Then on regular bases to check for new groups.

# Choose what types of media you want to download. 
ENABLE_MOVIES = True        
ENABLE_SERIES = True


# ------ SUB-FOLDERS ------
FOLDER_MAP = {
    "movies": "Movies",
    "series": "Series",
}

# Use season folders
SERIES_USE_SEASON_FOLDERS = True


# ------- ADVANCED SETTINGS BELOW -------

# -------- NAME NORMALIZATION --------
# Converts inconsistent or cluttered names into a clean, consistent format by removing unwanted tags and standardizing characters.
ENABLE_NAME_NORMALIZATION = True
REMOVE_NAME_TOKENS = [
    "PRE",
    "SE",
    "MULTI-SUB",
    "KIDS",
    "MULTI-AUDIO",
    "Multi-Sub",
    "Multi-Subs",
    "AR-SUB",
    "Only On 4K Devices",
    "IMDB",
]


# -------- URL REWRITE -------- 
# Used to replace link. Example if you are using a proxy.
ENABLE_URL_REWRITE = False
URL_REWRITE_FROM = "http://iptv.com:1234"
URL_REWRITE_TO   = "https://mynewurl.com"

# -------- CPU WORKLOAD --------

WORKER_MODE = "percent"   # "percent" | "fixed" | "auto"
CPU_USAGE_PERCENT = 60    # used if mode = "percent"
FIXED_WORKERS = 12        # used if mode = "fixed"

# ===============END USER SETTINGS ================



TIMEOUT = 30

SERIES_REGEX = re.compile(
    r'^(?P<show>.+?)\s*[-._ ]*\s*S(?P<season>\d{1,2})E\d{1,2}\b',
    re.IGNORECASE
)


# -------- GLOBALS --------
created_dirs = set()
dir_lock = Lock()


# -------- HELPERS --------
def get_attr(extinf, attr):
    m = re.search(r'%s="([^"]*)"' % attr, extinf)
    return m.group(1) if m else ""


def load_whitelist(path):
    allowed = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            allowed.add(line)
    return allowed

SERIES_EP_REGEX = re.compile(r'\bS\d{1,2}E\d{1,2}\b', re.IGNORECASE)

def classify(extinf):
    group = get_attr(extinf, "group-title")
    tvg_name = get_attr(extinf, "tvg-name")

    # Movies (explicit VOD)
    if group and "VOD" in group.upper():
        return "movies"

    # Series (episode pattern)
    if tvg_name and SERIES_EP_REGEX.search(tvg_name):
        return "series"

    # Ignore everything else (incl live TV)
    return None

# ==DO NOT USE (still here for fallback)
#def classify(extinf):
#    tvg_id = get_attr(extinf, "tvg-id")
#    group  = get_attr(extinf, "group-title")
#
#    if tvg_id:
#        return "live"
#    if group.startswith("Series:"):
#        return "series"
#    if group.startswith("VOD:"):
#        return "movies"
#    return None


def normalize_name(name):
    name = name.strip()

    if not ENABLE_NAME_NORMALIZATION:
        return name

    name = re.sub(r'[\\/*?:"<>|]', "_", name)
    name = name.replace("[", "(").replace("]", ")")

    for token in REMOVE_NAME_TOKENS:
        if not token:
            continue
        pattern = r'\b{}\b'.format(re.escape(token))
        name = re.sub(pattern, "", name, flags=re.IGNORECASE)

    name = name.replace("_", " ")
    name = re.sub(r'\(\s*\)', '', name)
    name = re.sub(r"\s{2,}", " ", name)

    return name.strip()


def ensure_dir(path):
    d = path.resolve()
    with dir_lock:
        if d in created_dirs:
            return
        d.mkdir(parents=True, exist_ok=True)
        created_dirs.add(d)


def write_strm(path, url):
    ensure_dir(path.parent)

    if not path.exists():
        with open(path, "w", encoding="utf-8") as f:
            f.write(url + "\n")
        return "created"

    with open(path, "r", encoding="utf-8") as f:
        existing = f.read().strip()

    if existing == url:
        return "unchanged"

    with open(path, "w", encoding="utf-8") as f:
        f.write(url + "\n")
    return "updated"


def download_m3u(url, dest):
    headers = {
        "User-Agent": "VLC/3.0.20 LibVLC/3.0.20",
        "Accept": "*/*",
        "Connection": "close",
    }

    last_error = None

    for attempt in range(1, 4):
        try:
            resp = requests.get(
                url,
                headers=headers,
                stream=True,
                timeout=TIMEOUT
            )
            resp.raise_for_status()

            with open(dest, "w", encoding="utf-8", errors="ignore") as f:
                for line in resp.iter_lines():
                    if not line:
                        continue
                    if isinstance(line, bytes):
                        line = line.decode("utf-8", errors="ignore")
                    f.write(line + "\n")

            return "downloaded"

        except requests.exceptions.RequestException as e:
            last_error = e
            print(f"Download failed (attempt {attempt}/3), retrying...")
            time.sleep(3 * attempt)

    if Path(dest).exists():
        print("WARNING: Using existing local M3U due to download failure")
        return "cached"

    raise RuntimeError(
        "Failed to download M3U and no local copy exists"
    ) from last_error

def rewrite_url(url):
    if not ENABLE_URL_REWRITE:
        return url

    if url.startswith(URL_REWRITE_FROM):
        return URL_REWRITE_TO + url[len(URL_REWRITE_FROM):]

    return url


def calculate_workers():
    cores = os.cpu_count() or 1

    if WORKER_MODE == "fixed":
        return max(1, FIXED_WORKERS)

    if WORKER_MODE == "percent":
        percent = max(1, min(CPU_USAGE_PERCENT, 100))
        return max(1, int(cores * (percent / 100.0)))

    # auto
    return cores

# ================= MAIN =================

start = time.perf_counter()

print("Downloading M3U file...")
m3u_status = download_m3u(M3U_URL, LOCAL_M3U_FILE)

print("Reading M3U file...")
with open(LOCAL_M3U_FILE, "r", encoding="utf-8", errors="ignore") as f:
    lines = [line.strip() for line in f if line.strip()]

allowed_groups = load_whitelist(WHITELIST_FILE)

expected_files = set()
stats = {"created": 0, "updated": 0, "unchanged": 0}
deleted_files = 0
deleted_dirs = 0

tasks = []
current_extinf = None

workers = calculate_workers()

with ThreadPoolExecutor(max_workers=workers) as executor:
    for line in lines:
        if line.startswith("#EXTINF"):
            current_extinf = line
            continue

        if line.startswith("#") or not current_extinf:
            continue

        url = rewrite_url(line)
        extinf = current_extinf
        current_extinf = None

        group = get_attr(extinf, "group-title")
        if group not in allowed_groups:
            continue

        content_type = classify(extinf)
        if not content_type:
            continue

        if (
            (content_type == "movies" and not ENABLE_MOVIES) or
            (content_type == "series" and not ENABLE_SERIES)
        ):
            continue

        tvg_name = get_attr(extinf, "tvg-name")
        if not tvg_name:
            continue

        base_dir = Path(OUTPUT_BASE) / FOLDER_MAP[content_type]

        if content_type == "series" and SERIES_USE_SEASON_FOLDERS:
            m = SERIES_REGEX.match(tvg_name)
            if m:
                show = normalize_name(m.group("show"))
                season = "Season %02d" % int(m.group("season"))
                target_dir = base_dir / show / season
            else:
                target_dir = base_dir
        else:
            target_dir = base_dir

        filename = normalize_name(tvg_name) + ".strm"
        strm_path = target_dir / filename

        expected_files.add(strm_path.resolve())

        tasks.append(
            executor.submit(write_strm, strm_path, url)
        )

    for future in as_completed(tasks):
        result = future.result()
        stats[result] += 1


# -------- SYNC DELETE --------
base = Path(OUTPUT_BASE)

for file in base.rglob("*.strm"):
    if file.resolve() not in expected_files:
        file.unlink()
        deleted_files += 1

for d in sorted(base.rglob("*"), reverse=True):
    if d.is_dir() and not any(d.iterdir()):
        d.rmdir()
        deleted_dirs += 1


elapsed = time.perf_counter() - start


# ================= SUMMARY =================

print("Completed")
print(f"  M3U source: {'Downloaded new file' if m3u_status == 'downloaded' else 'Used cached file'}")
print("  Created:     %d" % stats["created"])
print("  Updated: %d" % stats["updated"])
print("  Unchanged: %d" % stats["unchanged"])
print("  Deleted files:  %d" % deleted_files)
#print("  Deleted folders: %d" % deleted_dirs)
print("  Elapsed time: %.2f seconds" % elapsed)
